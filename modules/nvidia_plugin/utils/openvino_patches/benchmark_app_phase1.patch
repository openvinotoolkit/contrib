diff --git a/samples/cpp/benchmark_app/CMakeLists.txt b/samples/cpp/benchmark_app/CMakeLists.txt
index 4cb34fe1f4..792f4579c5 100644
--- a/samples/cpp/benchmark_app/CMakeLists.txt
+++ b/samples/cpp/benchmark_app/CMakeLists.txt
@@ -10,7 +10,7 @@ file (GLOB HDR ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp)
 ie_add_sample(NAME ${TARGET_NAME}
               SOURCES ${SRC}
               HEADERS ${HDR}
-              DEPENDENCIES ${GFLAGS_TARGET} format_reader ie_samples_utils)
+              DEPENDENCIES ${GFLAGS_TARGET} format_reader ie_samples_utils openvino_nvidia_gpu_plugin)
 
 # Required nlohmann_json dependency
 
diff --git a/samples/cpp/benchmark_app/benchmark_app.hpp b/samples/cpp/benchmark_app/benchmark_app.hpp
index 6ea383f5e4..fab5636d01 100644
--- a/samples/cpp/benchmark_app/benchmark_app.hpp
+++ b/samples/cpp/benchmark_app/benchmark_app.hpp
@@ -257,6 +257,9 @@ static constexpr char inference_only_message[] =
     " To enable full mode for static models pass \"false\" value to this argument:"
     " ex. \"-inference_only=false\".\n";
 
+/// @brief message for optimization mode
+static const char op_bench_mode_message[] = "Optional. Operator level benchmark on particular device";
+
 /// @brief Define flag for showing help message <br>
 DEFINE_bool(h, false, help_message);
 
@@ -387,6 +390,9 @@ DEFINE_string(scale_values, "", scale_values_message);
 /// @brief Define flag for inference only mode <br>
 DEFINE_bool(inference_only, true, inference_only_message);
 
+/// @brief Optimization mode for particular device
+DEFINE_bool(op_bench, false, op_bench_mode_message);
+
 /**
  * @brief This function show a help message
  */
@@ -428,6 +434,7 @@ static void show_usage() {
     std::cout << "    -json_stats             " << json_stats_message << std::endl;
     std::cout << "    -exec_graph_path        " << exec_graph_path_message << std::endl;
     std::cout << "    -pc                     " << pc_message << std::endl;
+    std::cout << "    -op_bench                 " << op_bench_mode_message << std::endl;
     std::cout << "    -pcsort                 " << pc_sort_message << std::endl;
     std::cout << "    -pcseq                  " << pcseq_message << std::endl;
     std::cout << "    -dump_config            " << dump_config_message << std::endl;
diff --git a/samples/cpp/benchmark_app/main.cpp b/samples/cpp/benchmark_app/main.cpp
index 5be1d8eb9d..a48a835e10 100644
--- a/samples/cpp/benchmark_app/main.cpp
+++ b/samples/cpp/benchmark_app/main.cpp
@@ -604,6 +604,13 @@ int main(int argc, char* argv[]) {
                     }
                 }
                 device_nstreams.erase(device);
+            } else if (device == "NVIDIA") {
+                setThroughputStreams();
+                if (isFlagSetInCommandLine("op_bench")) {
+                  device_config["NVIDIA_OPERATION_BENCHMARK"] = FLAGS_op_bench ?
+                                                              std::string("NVIDIA_YES") :
+                                                              std::string("NVIDIA_NO");
+                }
             }
         }
 
