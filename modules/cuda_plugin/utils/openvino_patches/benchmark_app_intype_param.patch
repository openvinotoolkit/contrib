diff --git a/inference-engine/samples/benchmark_app/benchmark_app.hpp b/inference-engine/samples/benchmark_app/benchmark_app.hpp
index 9ac25bb..fe3f80e 100644
--- a/inference-engine/samples/benchmark_app/benchmark_app.hpp
+++ b/inference-engine/samples/benchmark_app/benchmark_app.hpp
@@ -108,6 +108,10 @@ static const char layout_message[] = "Optional. Prompts how network layouts shou
 // @brief message for quantization bits
 static const char gna_qb_message[] = "Optional. Weight bits for quantization:  8 or 16 (default)";
 
+/// @brief message for #threads for CPU inference
+static const char input_type_message[] = "Optional. Input type among [U8, FP16, FP32]";
+
+
 /// @brief Define flag for showing help message <br>
 DEFINE_bool(h, false, help_message);
 
@@ -147,6 +151,9 @@ DEFINE_uint32(t, 0, execution_time_message);
 /// @brief Number of infer requests in parallel
 DEFINE_uint32(nireq, 0, infer_requests_count_message);
 
+/// @brief Input type
+DEFINE_string(input_type, "U8", input_type_message);
+
 /// @brief Number of threads to use for inference on the CPU in throughput mode (also affects Hetero cases)
 DEFINE_uint32(nthreads, 0, infer_num_threads_message);
 
@@ -232,6 +239,7 @@ static void showUsage() {
     std::cout << "    -report_folder            " << report_folder_message << std::endl;
     std::cout << "    -exec_graph_path          " << exec_graph_path_message << std::endl;
     std::cout << "    -pc                       " << pc_message << std::endl;
+    std::cout << "    -input_type               " << input_type_message << std::endl;
 #ifdef USE_OPENCV
     std::cout << "    -dump_config              " << dump_config_message << std::endl;
     std::cout << "    -load_config              " << load_config_message << std::endl;
diff --git a/inference-engine/samples/benchmark_app/main.cpp b/inference-engine/samples/benchmark_app/main.cpp
index a786eef..c7f5bff 100644
--- a/inference-engine/samples/benchmark_app/main.cpp
+++ b/inference-engine/samples/benchmark_app/main.cpp
@@ -374,7 +374,15 @@ int main(int argc, char *argv[]) {
             next_step();
 
             for (auto& item : inputInfo) {
-                if (app_inputs_info.at(item.first).isImage()) {
+                if (FLAGS_input_type == "FP16") {
+                    /** Set the precision of input data provided by the user, should be called before load of the network to the device **/
+                    app_inputs_info.at(item.first).precision = Precision::FP16;
+                    item.second->setPrecision(app_inputs_info.at(item.first).precision);
+                } else if (FLAGS_input_type == "FP32") {
+                    /** Set the precision of input data provided by the user, should be called before load of the network to the device **/
+                    app_inputs_info.at(item.first).precision = Precision::FP32;
+                    item.second->setPrecision(app_inputs_info.at(item.first).precision);
+                } else if (app_inputs_info.at(item.first).isImage() || FLAGS_input_type == "FP16") {
                     /** Set the precision of input data provided by the user, should be called before load of the network to the device **/
                     app_inputs_info.at(item.first).precision = Precision::U8;
                     item.second->setPrecision(app_inputs_info.at(item.first).precision);
