diff --git a/inference-engine/samples/benchmark_app/CMakeLists.txt b/inference-engine/samples/benchmark_app/CMakeLists.txt
index a74294a0f..8acaf9306 100644
--- a/inference-engine/samples/benchmark_app/CMakeLists.txt
+++ b/inference-engine/samples/benchmark_app/CMakeLists.txt
@@ -10,3 +10,4 @@ ie_add_sample(NAME benchmark_app
               HEADERS ${HDR}
               DEPENDENCIES format_reader
               OPENCV_DEPENDENCIES imgcodecs)
+add_dependencies(benchmark_app CUDAPlugin)
\ No newline at end of file
diff --git a/inference-engine/samples/benchmark_app/benchmark_app.hpp b/inference-engine/samples/benchmark_app/benchmark_app.hpp
index 9ac25bb82..16bfa0f20 100644
--- a/inference-engine/samples/benchmark_app/benchmark_app.hpp
+++ b/inference-engine/samples/benchmark_app/benchmark_app.hpp
@@ -53,6 +53,9 @@ static const char infer_num_streams_message[] = "Optional. Number of streams to
 /// @brief message for enforcing of BF16 execution where it is possible
 static const char enforce_bf16_message[] = "Optional. Enforcing of floating point operations execution in bfloat16 precision where it is acceptable.";
 
+/// @brief message for optimization mode
+static const char optimize_mode_message[] = "Optional. Optimize mode for particular device";
+
 /// @brief message for user library argument
 static const char custom_cpu_library_message[] = "Required for CPU custom layers. Absolute path to a shared library with the kernels implementations.";
 
@@ -108,6 +111,10 @@ static const char layout_message[] = "Optional. Prompts how network layouts shou
 // @brief message for quantization bits
 static const char gna_qb_message[] = "Optional. Weight bits for quantization:  8 or 16 (default)";
 
+/// @brief message for #input_type
+static const char input_type_message[] = "Optional. Input type among [U8, FP16, FP32]";
+
+
 /// @brief Define flag for showing help message <br>
 DEFINE_bool(h, false, help_message);
 
@@ -147,6 +154,9 @@ DEFINE_uint32(t, 0, execution_time_message);
 /// @brief Number of infer requests in parallel
 DEFINE_uint32(nireq, 0, infer_requests_count_message);
 
+/// @brief Input type
+DEFINE_string(input_type, "U8", input_type_message);
+
 /// @brief Number of threads to use for inference on the CPU in throughput mode (also affects Hetero cases)
 DEFINE_uint32(nthreads, 0, infer_num_threads_message);
 
@@ -156,6 +166,9 @@ DEFINE_string(nstreams, "", infer_num_streams_message);
 /// @brief Enforces bf16 execution with bfloat16 precision on systems having this capability
 DEFINE_bool(enforcebf16, false, enforce_bf16_message);
 
+/// @brief Optimization mode for particular device
+DEFINE_bool(optimize, false, optimize_mode_message);
+
 /// @brief Define parameter for batch size <br>
 /// Default is 0 (that means don't specify)
 DEFINE_uint32(b, 0, batch_size_message);
@@ -232,6 +245,8 @@ static void showUsage() {
     std::cout << "    -report_folder            " << report_folder_message << std::endl;
     std::cout << "    -exec_graph_path          " << exec_graph_path_message << std::endl;
     std::cout << "    -pc                       " << pc_message << std::endl;
+    std::cout << "    -input_type               " << input_type_message << std::endl;
+    std::cout << "    -optimize                 " << optimize_mode_message << std::endl;
 #ifdef USE_OPENCV
     std::cout << "    -dump_config              " << dump_config_message << std::endl;
     std::cout << "    -load_config              " << load_config_message << std::endl;
diff --git a/inference-engine/samples/benchmark_app/main.cpp b/inference-engine/samples/benchmark_app/main.cpp
index a786eef50..03ed81321 100644
--- a/inference-engine/samples/benchmark_app/main.cpp
+++ b/inference-engine/samples/benchmark_app/main.cpp
@@ -286,6 +286,13 @@ int main(int argc, char *argv[]) {
 
                 if (isFlagSetInCommandLine("nthreads"))
                     device_config[GNA_CONFIG_KEY(LIB_N_THREADS)] = std::to_string(FLAGS_nthreads);
+            } else if (device == "CUDA") {
+                setThroughputStreams();
+                if (isFlagSetInCommandLine("optimize")) {
+                  device_config["CUDA_OPTIMIZE"] = FLAGS_optimize ?
+                                                   std::string("CUDA_YES") :
+                                                   std::string("CUDA_NO");
+                }
             } else {
                 std::vector<std::string> supported_config_keys = ie.GetMetric(device, METRIC_KEY(SUPPORTED_CONFIG_KEYS));
                 auto supported = [&] (const std::string& key) {
@@ -374,7 +381,15 @@ int main(int argc, char *argv[]) {
             next_step();
 
             for (auto& item : inputInfo) {
-                if (app_inputs_info.at(item.first).isImage()) {
+                if (FLAGS_input_type == "FP16") {
+                    /** Set the precision of input data provided by the user, should be called before load of the network to the device **/
+                    app_inputs_info.at(item.first).precision = Precision::FP16;
+                    item.second->setPrecision(app_inputs_info.at(item.first).precision);
+                } else if (FLAGS_input_type == "FP32") {
+                    /** Set the precision of input data provided by the user, should be called before load of the network to the device **/
+                    app_inputs_info.at(item.first).precision = Precision::FP32;
+                    item.second->setPrecision(app_inputs_info.at(item.first).precision);
+                } else if (app_inputs_info.at(item.first).isImage() || FLAGS_input_type == "FP16") {
                     /** Set the precision of input data provided by the user, should be called before load of the network to the device **/
                     app_inputs_info.at(item.first).precision = Precision::U8;
                     item.second->setPrecision(app_inputs_info.at(item.first).precision);
