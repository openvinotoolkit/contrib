diff --git a/inference-engine/samples/benchmark_app/CMakeLists.txt b/inference-engine/samples/benchmark_app/CMakeLists.txt
index b37495e5e..9dc5da800 100644
--- a/inference-engine/samples/benchmark_app/CMakeLists.txt
+++ b/inference-engine/samples/benchmark_app/CMakeLists.txt
@@ -10,3 +10,4 @@ ie_add_sample(NAME benchmark_app
               HEADERS ${HDR}
               DEPENDENCIES format_reader ie_samples_utils
               OPENCV_DEPENDENCIES core)
+add_dependencies(benchmark_app CUDAPlugin)
diff --git a/inference-engine/samples/benchmark_app/benchmark_app.hpp b/inference-engine/samples/benchmark_app/benchmark_app.hpp
index af18c908e..d55baa34c 100644
--- a/inference-engine/samples/benchmark_app/benchmark_app.hpp
+++ b/inference-engine/samples/benchmark_app/benchmark_app.hpp
@@ -62,6 +62,9 @@ static const char enforce_bf16_message[] = "Optional. By default floating point
                                            "                                  'true'  - enable  bfloat16 regardless of platform support\n"
                                            "                                  'false' - disable bfloat16 regardless of platform support";
 
+/// @brief message for optimization mode
+static const char optimize_mode_message[] = "Optional. Optimize mode for particular device";
+
 /// @brief message for user library argument
 static const char custom_cpu_library_message[] = "Required for CPU custom layers. Absolute path to a shared library with the kernels "
                                                  "implementations.";
@@ -192,6 +195,9 @@ DEFINE_string(nstreams, "", infer_num_streams_message);
 /// @brief Enforces bf16 execution with bfloat16 precision on systems having this capability
 DEFINE_bool(enforcebf16, false, enforce_bf16_message);
 
+/// @brief Optimization mode for particular device
+DEFINE_bool(optimize, false, optimize_mode_message);
+
 /// @brief Define parameter for batch size <br>
 /// Default is 0 (that means don't specify)
 DEFINE_uint32(b, 0, batch_size_message);
@@ -288,6 +294,7 @@ static void showUsage() {
     std::cout << "    -report_folder            " << report_folder_message << std::endl;
     std::cout << "    -exec_graph_path          " << exec_graph_path_message << std::endl;
     std::cout << "    -pc                       " << pc_message << std::endl;
+    std::cout << "    -optimize                 " << optimize_mode_message << std::endl;
 #ifdef USE_OPENCV
     std::cout << "    -dump_config              " << dump_config_message << std::endl;
     std::cout << "    -load_config              " << load_config_message << std::endl;
diff --git a/inference-engine/samples/benchmark_app/main.cpp b/inference-engine/samples/benchmark_app/main.cpp
index cd7ddc641..c638f4f4e 100644
--- a/inference-engine/samples/benchmark_app/main.cpp
+++ b/inference-engine/samples/benchmark_app/main.cpp
@@ -295,6 +295,13 @@ int main(int argc, char* argv[]) {
 
                 if (isFlagSetInCommandLine("nthreads"))
                     device_config[GNA_CONFIG_KEY(LIB_N_THREADS)] = std::to_string(FLAGS_nthreads);
+            } else if (device == "CUDA") {
+                setThroughputStreams();
+                if (isFlagSetInCommandLine("optimize")) {
+                  device_config["CUDA_OPTIMIZE"] = FLAGS_optimize ?
+                                                   std::string("CUDA_YES") :
+                                                   std::string("CUDA_NO");
+                }
             } else {
                 std::vector<std::string> supported_config_keys = ie.GetMetric(device, METRIC_KEY(SUPPORTED_CONFIG_KEYS));
                 auto supported = [&](const std::string& key) {
