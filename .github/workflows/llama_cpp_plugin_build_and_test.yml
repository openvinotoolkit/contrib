name: llama_cpp_plugin_build_and_test

on:
  pull_request:
    types:
      - opened
      - reopened
      - synchronize
    paths:
      - 'modules/llama_cpp_plugin/**'

jobs:
  build_ubuntu20:
    runs-on: ubuntu-20.04
    steps:
      - name: Setup cmake
        uses: jwlawson/actions-setup-cmake@v1.14
        with:
          cmake-version: '3.24.x'

      - name: Checkout openvino_contrib
        uses: actions/checkout@v3
        with:
          submodules: recursive
          path: openvino_contrib

      - name: Checkout openvino
        uses: actions/checkout@v3
        with:
          submodules: recursive
          repository: vshampor/openvino
          ref: llama_cpp_mod
          path: openvino

      - name: CMake - configure
        run: cmake -B build -DCMAKE_BUILD_TYPE=Release -DOPENVINO_EXTRA_MODULES=${{ github.workspace }}/openvino_contrib/modules -DBUILD_java_api=OFF -DBUILD_nvidia_plugin=OFF -DBUILD_custom_operations=OFF -DBUILD_openvino_code=OFF -DBUILD_token_merging=OFF -DENABLE_TESTS=ON -DENABLE_FUNCTIONAL_TESTS=ON -DENABLE_PLUGINS_XML=ON openvino

      - name: CMake - build
        run: cmake --build build -j`nproc` -- llama_cpp_plugin llama_cpp_e2e_tests


      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build_artifacts
          path: ${{ github.workspace }}/openvino/bin/intel64/Release/

  test_ubuntu20:
    needs: build_ubuntu20
    runs-on: ubuntu-20.04
    steps:
      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build_artifacts
          path: ${{ github.workspace }}/binaries

      - name: Prepare test data - checkout llama.cpp repo
        uses: actions/checkout@v3
        with:
          repository: ggerganov/llama.cpp
          path: llama.cpp

      - name: Prepare test data - convert test model files
        run: |
          pip install -r llama.cpp/requirements/requirements-convert-hf-to-gguf.txt
          huggingface-cli download gpt2 model.safetensors tokenizer.json tokenizer_config.json vocab.json config.json merges.txt --local-dir hf_gpt2
          mkdir -p ${{ github.workspace }}/test_data
          python3 llama.cpp/convert-hf-to-gguf.py hf_gpt2 --outtype f32 --outfile ${{ github.workspace }}/test_data/gpt2.gguf

      - name: Run E2E tests
        run: |
          chmod +x ${{ github.workspace }}/binaries/llama_cpp_e2e_tests
          export LD_LIBRARY_PATH=${{ github.workspace }}/binaries
          ${{ github.workspace }}/binaries/llama_cpp_e2e_tests
